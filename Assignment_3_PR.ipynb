{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment#3_PR.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "LAJ4mKwHILi2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Testing for GPU availability\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sPGagDOYcZfF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -q keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ot7AEmywdT4g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### **To avoid tedious debates with the TA. Dina, we have to print all outputs of \"each\" step regardless of the importance of the step.** @Zyad\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "s_B07ZTYF-WI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Installing database file/s\n",
        "!wget http://104.197.136.14/ds/2016.04/2016.04C.multisnr.pkl.bz2\n",
        "!bzip2 -d 2016.04C.multisnr.pkl.bz2\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PplOSDykBPHt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The classes Sequential and Dense are used to specify the nodes, connections, specifications of the neural network.\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YrPJO_9avMIQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import numpy\n",
        "\n",
        "with open('2016.04C.multisnr.pkl', 'rb') as f:\n",
        "    u = pickle._Unpickler(f)\n",
        "    u.encoding = 'latin1'\n",
        "    p = u.load()\n",
        "    print(p[\"QPSK\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zkn_UmcKxYFB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sum=0\n",
        "for x in p.values():\n",
        "  sum+=x.shape[0]\n",
        "print(sum)\n",
        "# for key,value in p.items():\n",
        "#   if key[0]=='PAM4':\n",
        "#     sum+=value.shape[0]\n",
        "  \n",
        "# print(sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oXJ87eetweeR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(type(p))\n",
        "data = {}\n",
        "allm = []\n",
        "for k in p.keys():\n",
        "  data[k[0]] = {}\n",
        "  allm.append(k[0])\n",
        "mod = sorted(set(allm))\n",
        "print(mod)\n",
        "print(data)\n",
        "print(len(allm))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-B9LvXk3kDb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import zeros, newaxis\n",
        "\n",
        "for m in mod:\n",
        "  for k in p.keys():\n",
        "    if k[0] == m :\n",
        "      for sig in range(len(p[k])):\n",
        "        a = np.array(p[k][sig][0])[:, newaxis]\n",
        "        b = np.array(p[k][sig][1])[:, newaxis]\n",
        "        if k[1] not in data[k[0]]:\n",
        "          data[k[0]][k[1]] = []\n",
        "        data[k[0]][k[1]].append([a,b])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE032gi2gGao",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2 dimentions vectors because complex values a vector for real value and imaginary value"
      ]
    },
    {
      "metadata": {
        "id": "AkgLLbnZ8pPw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "dat = []\n",
        "#Training data and labels \n",
        "X = []\n",
        "Y = []\n",
        "#dictionary contating samples for each snr, key is snr and value is list of samples having this snr\n",
        "samples_per_snr = {}\n",
        "#dictonary accessed by snr and gives labels of mod types ,snr is index\n",
        "labels_ofSamples_per_snr = {}\n",
        "#dictionary saving all one-hot encodes of different modulation\n",
        "mval = {}\n",
        "count = 0\n",
        "\n",
        "#changed variables z one_hot_label, x -> samples_per_snr , y->labels_ofSamples_per_snr  both used for testing\n",
        "\n",
        "#loop all modulation techniques ex QPSK,BSK...etc\n",
        "for m in mod:\n",
        "  one_hot_label = np.zeros((len(mod),))\n",
        "  one_hot_label[count] = 1     \n",
        "  mval[m] = one_hot_label\n",
        "  #repeat for each snr for modulation technique m\n",
        "  for snr in data[m]:\n",
        "    #print(m,snr)\n",
        "    dat = data[m][snr]\n",
        "    #train with 2/3 of data from start\n",
        "    #1/3 for each channel ??\n",
        "    for d in dat[:int(len(dat)//1.5)]:\n",
        "      X.append(d)\n",
        "      Y.append(one_hot_label)\n",
        "      #loop the remaining 1/3 of data to test with them\n",
        "      for d in dat[int(len(dat)//1.5):]:\n",
        "        #only performed once to set the 20 snrs as keys in dictionary\n",
        "        #each value for key is list of samples having this snrs\n",
        "        if not snr in samples_per_snr:\n",
        "          samples_per_snr[snr] = []\n",
        "          labels_ofSamples_per_snr[snr] = []\n",
        "        #the real loop body :\n",
        "        samples_per_snr[snr].append(d)\n",
        "        labels_ofSamples_per_snr[snr].append(one_hot_label)\n",
        "  #increment loop counter\n",
        "  count += 1 \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nP9-4etfBBCP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(X))\n",
        "print(len(Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jZHqyJehCZSc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# cm = confusion_matrix(..., ...) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xKrtupPrB-eh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Sample Test\n",
        "'''\n",
        "\n",
        "input_dim_ = 30 #@param {type:\"integer\"}\n",
        "\n",
        "# Initialising the ANN\n",
        "classifier = Sequential()\n",
        "\n",
        "# Initialising input layer\n",
        "classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = input_dim_))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
        "\n",
        "# Compiling the ANN\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}